{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d298350",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sudo snap remove curl\n",
    "sudo apt install curl\n",
    "curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "conda create -n ollamapy310 python=3.10\n",
    "\n",
    "conda activate ollamapy310\n",
    "\n",
    " ollama pull  zephyr\n",
    "\n",
    " Zephyr is 4.1 GB\n",
    "\n",
    "pip install chromadb\n",
    "pip install langchain\n",
    "pip install BeautifulSoup4\n",
    "pip install gpt4all\n",
    "pip install langchainhub\n",
    "pip install pypdf\n",
    "pip install chainlit\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.document_loaders import UnstructuredHTMLLoader, BSHTMLLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings  \n",
    "\n",
    "import os\n",
    "\n",
    "# we have to loop across all the folders\n",
    "\n",
    "\n",
    "resource_folders=os.listdir(\"resources\")\n",
    "if \".DS_Store\" in resource_folders:\n",
    "    resource_folders.remove(\".DS_Store\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8636b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"vectorstores\"):\n",
    "    os.mkdir(\"vectorstores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c918ee45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palestineBookAwards\n",
      "Justice For Some_ Law And The Question Of Palestine -- Noura Erakat -- 2019 -- Stanford University Press.pdf\n",
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n",
      "Psychoanalysis Under Occupation Practicing Resistance in Palestine-- Lara Sheehi & Stephen Sheehi.pdf\n",
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n",
      "********\n",
      "aupress\n",
      "Under_the_Nakba_Tree_Househ_2022.pdf\n",
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "for folder in resource_folders:\n",
    "    print(folder)\n",
    "    if not os.path.isdir(f\"vectorstores/{folder}\"):\n",
    "        os.mkdir(f\"vectorstores/{folder}\")\n",
    "    \n",
    "    files_specific=os.listdir(f\"resources/{folder}\")\n",
    "#     print(files_specific)\n",
    "    if \".DS_Store\" in files_specific:\n",
    "        files_specific.remove(\".DS_Store\")\n",
    "    for f in files_specific:\n",
    "        print(f)\n",
    "        pdf_path = f\"resources/{folder}/{f}\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents=loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        texts=text_splitter.split_documents(documents)\n",
    "        vectorstore = Chroma.from_documents(documents=texts, embedding=GPT4AllEmbeddings(),persist_directory=f\"vectorstores/{folder}/{f}\")      \n",
    "    print(\"*\"*8)\n",
    "    \n",
    "    # create the vector DB\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310llama",
   "language": "python",
   "name": "py310llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
